{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('D:\\proand\\predict\\model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('D:\\proand\\data\\ingredient.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_word_deletion(sentence, deletion_prob=0.1):\n",
    "    '''\n",
    "    Add Random Word Deletion for Data Augmentation\n",
    "    A sentence parameter is the values in ingredients column of the dataset\n",
    "    '''\n",
    "    if isinstance(sentence, str):\n",
    "        words = sentence.split()\n",
    "        new_words = [word for word in words if random.random() > deletion_prob]\n",
    "        if not new_words:\n",
    "            return sentence\n",
    "        else:\n",
    "            return ' '.join(new_words)\n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "def shuffle_ingredients(sentence):\n",
    "    '''\n",
    "    Shuffle the ingredients' orders for Data Augmentation\n",
    "    A sentence parameter is the values in ingredients column of the dataset\n",
    "    '''\n",
    "    if isinstance(sentence, str):\n",
    "        words = sentence.split()\n",
    "        random.shuffle(words)\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ingredients'] = df['ingredients'].apply(random_word_deletion)\n",
    "df['ingredients'] = df['ingredients'].apply(shuffle_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['ingredients'])\n",
    "encoded_ingredients = tokenizer.texts_to_sequences(df['ingredients'])\n",
    "max_length = max([len(seq) for seq in encoded_ingredients])\n",
    "padded_ingredients = pad_sequences(encoded_ingredients, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "ratings_encoded = label_encoder.fit_transform(df['rating'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_ingredients, ratings_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "7/7 [==============================] - 3s 137ms/step - loss: 1.6749 - accuracy: 0.3423 - val_loss: 1.5578 - val_accuracy: 0.4000\n",
      "Epoch 2/300\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.6607 - accuracy: 0.3333 - val_loss: 1.5578 - val_accuracy: 0.4000\n",
      "Epoch 3/300\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.6408 - accuracy: 0.3514 - val_loss: 1.5398 - val_accuracy: 0.5000\n",
      "Epoch 4/300\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.6337 - accuracy: 0.3333 - val_loss: 1.5357 - val_accuracy: 0.4000\n",
      "Epoch 5/300\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.6212 - accuracy: 0.3514 - val_loss: 1.5218 - val_accuracy: 0.4000\n",
      "Epoch 6/300\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.6385 - accuracy: 0.3423 - val_loss: 1.5328 - val_accuracy: 0.4500\n",
      "Epoch 7/300\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.6132 - accuracy: 0.3514 - val_loss: 1.5692 - val_accuracy: 0.5000\n",
      "Epoch 8/300\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.6092 - accuracy: 0.3243 - val_loss: 1.5456 - val_accuracy: 0.4500\n",
      "Epoch 9/300\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.5835 - accuracy: 0.3423 - val_loss: 1.5540 - val_accuracy: 0.4000\n",
      "Epoch 10/300\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.5701 - accuracy: 0.3784 - val_loss: 1.5659 - val_accuracy: 0.4000\n",
      "Epoch 11/300\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.5697 - accuracy: 0.3874 - val_loss: 1.5811 - val_accuracy: 0.3500\n",
      "Epoch 12/300\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.5287 - accuracy: 0.3874 - val_loss: 1.5691 - val_accuracy: 0.3500\n",
      "Epoch 13/300\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1.5569 - accuracy: 0.3874 - val_loss: 1.5724 - val_accuracy: 0.4000\n",
      "Epoch 14/300\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.5523 - accuracy: 0.4054 - val_loss: 1.5817 - val_accuracy: 0.4500\n",
      "Epoch 15/300\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.5268 - accuracy: 0.3964 - val_loss: 1.5874 - val_accuracy: 0.4000\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    x=[X_train, y_train, X_train],\n",
    "    y=y_train,\n",
    "    epochs=300,\n",
    "    batch_size=16,\n",
    "    validation_split=0.15,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\proand\\env\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# model.save('D:\\proand\\predict\\model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
